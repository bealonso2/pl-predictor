{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all matches from 2022-2023 season\n",
    "\n",
    "import requests\n",
    "\n",
    "# Get API token from .env file\n",
    "with open(\".env\") as f:\n",
    "  for line in f:\n",
    "    if \"FOOTBALL-DATA-API-KEY\" in line:\n",
    "      token = line.split(\"=\")[1].strip()\n",
    "      break\n",
    "\n",
    "headers = { \"X-Auth-Token\": token }\n",
    "\n",
    "def get_data_by_year(year):\n",
    "  uri = f\"https://api.football-data.org/v4/competitions/2021/matches?season={year}\"\n",
    "  response = requests.get(uri, headers=headers)\n",
    "  return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def build_data_by_year(year):\n",
    "    df = pd.DataFrame(get_data_by_year(year)[\"matches\"])\n",
    "\n",
    "    # Drop columns we don\"t need\n",
    "    df = df[[\"utcDate\", \"matchday\", \"homeTeam\", \"awayTeam\", \"score\"]]\n",
    "\n",
    "    # Parse the homeTeam and awayTeam and get the names or ids from the column\n",
    "    df[\"home\"] = df[\"homeTeam\"].apply(lambda x: x[\"name\"])\n",
    "    df[\"away\"] = df[\"awayTeam\"].apply(lambda x: x[\"name\"])\n",
    "    df = df.drop(columns=[\"homeTeam\", \"awayTeam\"])\n",
    "\n",
    "    # Get home team and away team scores from the dataframe, drop score column\n",
    "    df[\"homeScore\"] = df[\"score\"].apply(lambda x: x[\"fullTime\"][\"home\"])\n",
    "    df[\"awayScore\"] = df[\"score\"].apply(lambda x: x[\"fullTime\"][\"away\"])\n",
    "    df = df.drop(columns=[\"score\"])\n",
    "\n",
    "    # Convert utcDate to datetime\n",
    "    df[\"utcDate\"] = pd.to_datetime(df[\"utcDate\"])\n",
    "\n",
    "    # Function to convert camel case to title case\n",
    "    def camel_to_title(camel_str):\n",
    "        title_str = re.sub(\"([A-Z])\", r\" \\1\", camel_str)\n",
    "        return title_str.title()\n",
    "\n",
    "    # Apply the function to each column name\n",
    "    df.columns = [camel_to_title(col) for col in df.columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Get data for 2022-2023 season\n",
    "df = build_data_by_year(2022)\n",
    "\n",
    "# Get data for the 2023-2024 season\n",
    "df_2023 = build_data_by_year(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'simulate_match' from 'simulation_utils' (/Users/brianalonso/Documents/4 Archives/PL Predictor/simulation_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[248], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msimulation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simulate_match\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize elo ratings for each team\u001b[39;00m\n\u001b[1;32m      4\u001b[0m elo \u001b[38;5;241m=\u001b[39m {team: \u001b[38;5;241m1500\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m team \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHome\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()}\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'simulate_match' from 'simulation_utils' (/Users/brianalonso/Documents/4 Archives/PL Predictor/simulation_utils.py)"
     ]
    }
   ],
   "source": [
    "from simulation_utils import update_elo_win, update_elo_draw\n",
    "\n",
    "# Initialize elo ratings for each team\n",
    "elo = {team: 1500 for team in df[\"Home\"].unique()}\n",
    "\n",
    "# Process matches and update ELO ratings\n",
    "for index, row in df.iterrows():\n",
    "    home_team, away_team = row[\"Home\"], row[\"Away\"]\n",
    "    \n",
    "    if row[\"Home Score\"] > row[\"Away Score\"]:  # Home team won\n",
    "        elo[home_team], elo[away_team] = update_elo_win(elo[home_team], elo[away_team])\n",
    "    elif row[\"Away Score\"] > row[\"Home Score\"]:  # Away team won\n",
    "        elo[away_team], elo[home_team] = update_elo_win(elo[away_team], elo[home_team])\n",
    "    else: # Draw\n",
    "        elo[home_team], elo[away_team] = update_elo_draw(elo[home_team], elo[away_team])\n",
    "    \n",
    "    df.at[index, \"Home Elo\"] = elo[home_team]\n",
    "    df.at[index, \"Away Elo\"] = elo[away_team]\n",
    "\n",
    "# Determine outcomes: 3 for win, 1 for draw, 0 for loss\n",
    "df[\"Home Outcome\"] = 1\n",
    "df[\"Away Outcome\"] = 1\n",
    "df.loc[df[\"Home Score\"] > df[\"Away Score\"], \"Home Outcome\"] = 3\n",
    "df.loc[df[\"Home Score\"] > df[\"Away Score\"], \"Away Outcome\"] = 0\n",
    "df.loc[df[\"Away Score\"] > df[\"Home Score\"], \"Away Outcome\"] = 3\n",
    "df.loc[df[\"Away Score\"] > df[\"Home Score\"], \"Home Outcome\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation_utils import get_season_results\n",
    "\n",
    "results = get_season_results(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 71.05%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For simplicity, let\"s predict the home outcome based on Elo ratings\n",
    "x = df[[\"Home Elo\", \"Away Elo\"]]\n",
    "y = df[\"Home Outcome\"]\n",
    "\n",
    "# Split data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestClassifier(n_estimators=1000, max_depth=5, min_samples_split=5)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "val_score = model.score(x_val, y_val)\n",
    "print(f\"Validation accuracy: {val_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find teams that have been relegated/promoted by taking a difference of the two dataframes\n",
    "df_teams = pd.concat([df[\"Home\"], df[\"Away\"]]).unique()\n",
    "df_2023_teams = pd.concat([df_2023[\"Home\"], df_2023[\"Away\"]]).unique()\n",
    "\n",
    "relegated_teams = set(df_teams) - set(df_2023_teams)\n",
    "promoted_teams = set(df_2023_teams) - set(df_teams)\n",
    "teams_with_baseline = set(df_teams) & set(df_2023_teams)\n",
    "\n",
    "# Find the average ending ELO rating for the teams that have been relegated\n",
    "relegated_elo = results.loc[list(relegated_teams), \"Total Elo\"].mean()\n",
    "\n",
    "# Set the starting ELO rating for the promoted teams to the average ending ELO rating of the relegated teams\n",
    "elo = {team: relegated_elo for team in promoted_teams}\n",
    "\n",
    "# Set the starting ELO rating for the teams that have been in the league for both seasons to their ending ELO rating\n",
    "elo.update(results.loc[list(teams_with_baseline), \"Total Elo\"].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Against 2023-2024 Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating: 100%|██████████| 1000/1000 [12:00:24<00:00, 43.22s/season]   \n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from simulation_utils import simulate_and_get_results\n",
    "\n",
    "num_simulations = 1000\n",
    "# num_simulations = 20000\n",
    "\n",
    "# Initialize a list to store results\n",
    "seasons = []\n",
    "\n",
    "\n",
    "# Initialize a pool of workers\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    seasons = list(tqdm(executor.map(simulate_and_get_results, range(num_simulations), [df_2023]*num_simulations, [elo]*num_simulations, [model]*num_simulations, [scaler]*num_simulations),  total=num_simulations, desc='Simulating', unit='season'))\n",
    "\n",
    "# for i in tqdm(range(num_simulations), desc='Simulating', unit='season'):\n",
    "#     simulated_df = simulate_season(df_2023, elo)\n",
    "#     results = get_season_results(simulated_df)\n",
    "#     results[\"Season\"] = i\n",
    "#     seasons.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Results Compared to actual 2023-2024 Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results based on total outcome\n",
    "results = results.sort_values(\"Total Outcome\", ascending=False)\n",
    "\n",
    "# Get the place each team finished in the league\n",
    "results[\"Place\"] = range(1, len(results) + 1)\n",
    "\n",
    "# Get 2023 season results\n",
    "# Determine outcomes: 3 for win, 1 for draw, 0 for loss\n",
    "df_2023[\"Home Outcome\"] = 1\n",
    "df_2023[\"Away Outcome\"] = 1\n",
    "df_2023.loc[df_2023[\"Home Score\"] > df_2023[\"Away Score\"], \"Home Outcome\"] = 3\n",
    "df_2023.loc[df_2023[\"Home Score\"] > df_2023[\"Away Score\"], \"Away Outcome\"] = 0\n",
    "df_2023.loc[df_2023[\"Away Score\"] > df_2023[\"Home Score\"], \"Away Outcome\"] = 3\n",
    "df_2023.loc[df_2023[\"Away Score\"] > df_2023[\"Home Score\"], \"Home Outcome\"] = 0\n",
    "\n",
    "home_results = df_2023.groupby(\"Home\").agg({\"Home Outcome\": \"sum\"})\n",
    "away_results = df_2023.groupby(\"Away\").agg({\"Away Outcome\": \"sum\"})\n",
    "results_2023 = home_results.join(away_results, how=\"outer\").fillna(0)\n",
    "results_2023[\"Total Outcome\"] = results_2023[\"Home Outcome\"] + results_2023[\"Away Outcome\"]\n",
    "results_2023 = results_2023.sort_values(\"Total Outcome\", ascending=False)\n",
    "results_2023[\"Place\"] = range(1, len(results_2023) + 1)\n",
    "\n",
    "# For each season, get the place each team finished in the league\n",
    "for index, season_df in enumerate(seasons):\n",
    "    season_df[\"Place\"] = range(1, len(season_df) + 1)\n",
    "\n",
    "# Get the average place each team finished in the league\n",
    "average_results = pd.concat(seasons).groupby(\"Home\").agg({\"Place\": \"mean\"}).sort_values(\"Place\")\n",
    "\n",
    "# Get a mapping of team names to a list places they finished in the league\n",
    "team_place_mapping = {}\n",
    "for team in average_results.index:\n",
    "    team_place_mapping[team] = [season_df.loc[team, \"Place\"] for season_df in seasons]\n",
    "\n",
    "# Get the total number of seasons simulated\n",
    "total_seasons = len(seasons)\n",
    "\n",
    "# Get a mapping of times each team won the league\n",
    "team_win_mapping = {}\n",
    "for team in average_results.index:\n",
    "    team_win_mapping[team] = sum([season_df.loc[team, \"Place\"] == 1 for season_df in seasons]) / total_seasons\n",
    "\n",
    "# Get a mapping of times each team finished in the top 4\n",
    "team_top_4_mapping = {}\n",
    "for team in average_results.index:\n",
    "    team_top_4_mapping[team] = sum([season_df.loc[team, \"Place\"] <= 4 for season_df in seasons]) / total_seasons\n",
    "\n",
    "# Get a mapping of times each team finished in the bottom 3\n",
    "team_bottom_3_mapping = {}\n",
    "for team in average_results.index:\n",
    "    team_bottom_3_mapping[team] = sum([season_df.loc[team, \"Place\"] >= len(season_df) - 3 for season_df in seasons]) / total_seasons\n",
    "\n",
    "# Build a dataframe with the average place, times won, times in top 4, and times in bottom 3\n",
    "average_results[\"Win Premier League\"] = [team_win_mapping[team] for team in average_results.index]\n",
    "average_results[\"Top 4\"] = [team_top_4_mapping[team] for team in average_results.index]\n",
    "average_results[\"Bottom 3\"] = [team_bottom_3_mapping[team] for team in average_results.index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
